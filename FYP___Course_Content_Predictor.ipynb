{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TKzo6FJ3DA4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import math\n",
        "import io\n",
        "import gensim\n",
        "import csv\n",
        "import warnings\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U pip setuptools wheel\n",
        "# !pip install keybert\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# !python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "um_WdjNHt0dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhCxJBFG6WT5"
      },
      "outputs": [],
      "source": [
        "def cleaning_dataset(JD):\n",
        "# Convert to list\n",
        "  data = JD.values.tolist()\n",
        "  # Remove new line characters\n",
        "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "  # Remove Emails\n",
        "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "  # Remove distracting single quotes\n",
        "  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XRPuxPgSUuA"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "      # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "# Convert a document in a list of token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MOy2mhrSfMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfda83f0-d206-43a3-f3df-2bba5e738b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-11 13:04:39.056064: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    # \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ \n",
        "        not in ['-PRON-'] else '' for token in \n",
        "        doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "!python -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eHygTgmz1rk"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def vectorize(data_lemmatized):\n",
        "  vec = TfidfVectorizer(stop_words=\"english\")\n",
        "  X = vec.fit_transform(data_lemmatized)\n",
        "  features = vec.get_feature_names_out()\n",
        "  return [X,features]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET9t1dMF0ZTZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def topic_modeling(vector):\n",
        "  lda = LatentDirichletAllocation(n_components = 25, random_state=None, \n",
        "                                 doc_topic_prior = 1.25,topic_word_prior = 0.09)\n",
        "  # doc_topic_prior = alpha\n",
        "  # topic_word_prior = beta\n",
        "  theta = lda.fit_transform(vector)\n",
        "  return [theta,lda.components_,lda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td7MgNqj2eiL"
      },
      "outputs": [],
      "source": [
        "def document_per_topic(lda_component,features):\n",
        "  for tid, topic in enumerate(lda_component):\n",
        "    print('Topic ID: ', tid)\n",
        "    print('Word IDs: ', topic.argsort()[:-15:-1])\n",
        "    print('Word prob.: ', [topic[i] for i in topic.argsort()[:-15:-1]])\n",
        "    print('Words: ', [features  [i] for i in topic.argsort()[:-15:-1]])\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsmXtiuRmYU2"
      },
      "outputs": [],
      "source": [
        "def topic_per_word(theta,lda_component,features):\n",
        "  for doc_id, tid in enumerate(theta):\n",
        "      print(\"JD ID: \", doc_id)\n",
        "      print('Topic IDs: ', tid.argsort()[-10:-1])\n",
        "      for i in tid.argsort()[-10:-1]:\n",
        "        print(\"The words in the topic \",i, \"are: \",[features[j] for j in lda_component[i].argsort()[-20:-1]])\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYlnd4YeUhCQ"
      },
      "outputs": [],
      "source": [
        "def top_20_words_per_document(theta,lda_components,features):\n",
        "  dic_doc_words = {}\n",
        "  for doc_id, tid in enumerate(theta):\n",
        "      dic_doc_words[doc_id] = []\n",
        "      for i in tid.argsort()[-5:-1]:\n",
        "        n = 1\n",
        "        l = []\n",
        "        for j in lda_components[i].argsort()[-20:-1]:\n",
        "            if n < 20:\n",
        "              l.append(features[j])\n",
        "              n += 1\n",
        "        dic_doc_words[doc_id].append(l)\n",
        "  new_dic_doc = {}\n",
        "  for key,values in dic_doc_words.items():\n",
        "    text = \"\"\n",
        "    for value in values:\n",
        "      for v in value:\n",
        "        text += \" \"\n",
        "        text += v\n",
        "    new_dic_doc[key] = text\n",
        "  \n",
        "  return new_dic_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzmPNqshWavj"
      },
      "outputs": [],
      "source": [
        "def evaluation(lda,vector):\n",
        "  # Log Likelyhood: Higher the better\n",
        "  print(\"Log Likelihood: \", lda.score(vector))\n",
        "  # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "  print(\"Perplexity: \", lda.perplexity(vector))\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzJT9zhddJ5k"
      },
      "outputs": [],
      "source": [
        "def list_of_words(theta,lda_components):\n",
        "  dic_doc_words = {}\n",
        "  for doc_id, tid in enumerate(theta):\n",
        "      dic_doc_words[doc_id] = []\n",
        "      for i in tid.argsort()[-5:-1]:\n",
        "        n = 1\n",
        "        l = []\n",
        "        for j in lda_components[i].argsort()[-20:-1]:\n",
        "            if n < 20:\n",
        "              l.append(features[j])\n",
        "              n += 1\n",
        "        dic_doc_words[doc_id].append(l)\n",
        "  word_list = []\n",
        "  for key,value in dic_doc_words.items():\n",
        "    for v in value:\n",
        "      for v1 in v:\n",
        "        word_list.append(v1)\n",
        "  return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eit85RVJdVkw"
      },
      "outputs": [],
      "source": [
        "def JD_ID(jd):\n",
        "  dic_jd = {}\n",
        "  count = 0\n",
        "  for i in jd:\n",
        "    dic_jd[count] = i\n",
        "    count += 1\n",
        "  return dic_jd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSBRbfpmdQlw"
      },
      "outputs": [],
      "source": [
        "def merge_documents(corpus):\n",
        "  documents = []\n",
        "  for key,value in corpus.items():\n",
        "    l = value.split()\n",
        "    for i in l:\n",
        "      documents.append(i.lower())\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHhFUMxJelIg"
      },
      "outputs": [],
      "source": [
        "def topic_coherence(word_list,documents):\n",
        "  \n",
        "  ans = 0\n",
        "  count = 0\n",
        "  for word in word_list:\n",
        "    \n",
        "    wi = 0\n",
        "    wi_wj = 0\n",
        "    if len(word_list) != count+1:\n",
        "      next_word = word_list[count+1]\n",
        "      # print(next_word)\n",
        "      flag = False\n",
        "      for doc in documents:\n",
        "        if word == doc:\n",
        "          wi += 1  \n",
        "          flag = True\n",
        "        \n",
        "        if next_word == doc and flag == True:\n",
        "          flag = False\n",
        "          wi_wj += 1\n",
        "      \n",
        "        # print(\"iteration\")\n",
        "      count += 1\n",
        "      if wi == 0:\n",
        "          wi =1\n",
        "      sum = (wi_wj+1) / wi\n",
        "      ans += math.log(sum)\n",
        "      \n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def courses_dic(JD_title,JD_outlines):\n",
        "  JD_dic = {}\n",
        "  for i in range(len(JD_title)):\n",
        "    JD_dic[JD_title[i]] = JD_outlines[i]\n",
        "\n",
        "  new_JD_dic = {}\n",
        "  for title, outline in JD_dic.items():\n",
        "    if len(str(outline)) < 5:\n",
        "      pass\n",
        "    else:\n",
        "      new_JD_dic[title] = outline\n",
        "  return new_JD_dic"
      ],
      "metadata": {
        "id": "vsxFfpUSGXyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Jaccard_Similarity(doc1, doc2): \n",
        "    \n",
        "    # List the unique words in a document\n",
        "    words_doc1 = set(doc1.split()) \n",
        "    words_doc2 = set(doc2.split())\n",
        "    \n",
        "    # Find the intersection of words list of doc1 & doc2\n",
        "    intersection = words_doc1.intersection(words_doc2)\n",
        "\n",
        "    # Find the union of words list of doc1 & doc2\n",
        "    union = words_doc1.union(words_doc2)\n",
        "        \n",
        "    # Calculate Jaccard similarity score \n",
        "    # using length of intersection set divided by length of union set\n",
        "    return float(len(intersection)) / len(union)"
      ],
      "metadata": {
        "id": "crD_PeIzHY9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maintain_outline_list(list_out,outline,score):\n",
        "  if len(list_out) < 3:\n",
        "    return list_out.append([outline,score])\n",
        "\n",
        "  else:\n",
        "    for i in list_out:\n",
        "      if i[1] < score:\n",
        "        list_out.remove(i)\n",
        "        list_out.append([outline,score])\n",
        "        return list_out"
      ],
      "metadata": {
        "id": "VgACaSIKIwRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "def topic_for_course(dic_new_outline,list_out,title,job_desc,score):\n",
        "  if len(list_out) < 4:\n",
        "    dic_new_outline[title] = job_desc\n",
        "    return dic_new_outline\n",
        "\n",
        "  else:\n",
        "    for i in list_out:\n",
        "      count = 0\n",
        "      if i[1] < score:\n",
        "        del dic_new_outline[next(islice(dic_new_outline, count, None))]\n",
        "        dic_new_outline[title] = job_desc\n",
        "        count += 1\n",
        "      else:\n",
        "        count+= 1    \n",
        "    return dic_new_outline\n",
        "      "
      ],
      "metadata": {
        "id": "gdfKjbOUOEwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words_from_jaccard_similarity(top_words_per_document,JD_dic):\n",
        "  jaccard_similarity= {}\n",
        "  course_title_new_topics = {}\n",
        "  for key,job_desc in top_words_per_document.items():\n",
        "    max_score = False\n",
        "    course_titles = []\n",
        "    dic_outline = {}\n",
        "    for title,outlines in JD_dic.items():\n",
        "      score = Jaccard_Similarity(job_desc,outlines)\n",
        "      if (max_score < score):\n",
        "        max_score = score\n",
        "        maintain_outline_list(course_titles,title,max_score)\n",
        "        topic_for_course(dic_outline,course_titles,title,job_desc,max_score)\n",
        "    jaccard_similarity[key] = course_titles \n",
        "    course_title_new_topics[key] = dic_outline\n",
        "  return [jaccard_similarity,course_title_new_topics]"
      ],
      "metadata": {
        "id": "ai0ANh9tPV2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Cosine_Similarity(a, b):\n",
        "   return dot(a, b)/(norm(a)*norm(b))"
      ],
      "metadata": {
        "id": "B-VuHfSpi8Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_cvs_file_of_Similarity(name,top_words_per_document,similarity)\n",
        "# with open('Outline_new_content.csv','w',encoding='UTF8',newline=\"\") as f:\n",
        "#     header = ['Title','New Content']\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow(header)\n",
        "#     new_content = course_outline_new_content(top_words_per_document,similarity)\n",
        "#     for key,value in new_content.items():\n",
        "#         string = \"\"\n",
        "#         for v in value:\n",
        "#             string += v+', '\n",
        "#         writer.writerow([key,string])"
      ],
      "metadata": {
        "id": "GyOYoXHRY8U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words_from_cosine_similarity(top_words_per_document,JD_dic):\n",
        "  cosine_similarity= {}\n",
        "  course_title_new_topics = {}\n",
        "  for key,job_desc in top_words_per_document.items():\n",
        "    max_score = False\n",
        "    course_titles = []\n",
        "    dic_outline = {}\n",
        "    nlp_job_desc_vec = nlp(job_desc).vector\n",
        "    for title,outlines in new_JD_dic.items():\n",
        "\n",
        "      nlp_outline_vec = nlp(outlines).vector\n",
        "        \n",
        "      score = Cosine_Similarity(nlp_job_desc_vec, nlp_outline_vec)\n",
        "      if (max_score < score):\n",
        "        max_score = score\n",
        "        maintain_outline_list(course_titles,title,max_score)\n",
        "        topic_for_course(dic_outline,course_titles,title,job_desc,max_score)\n",
        "    cosine_similarity[key] = course_titles \n",
        "    course_title_new_topics[key] = dic_outline\n",
        "  return [cosine_similarity,course_title_new_topics]"
      ],
      "metadata": {
        "id": "vxBNK5eri9Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_occuring_course(similarity_dic):\n",
        "  similar_dic = {}\n",
        "  for key,value in similarity_dic.items():\n",
        "    for v in value:\n",
        "      if v[0] not in similar_dic:\n",
        "        similar_dic[v[0]]=0\n",
        "      else:\n",
        "        similar_dic[v[0]]+=1\n",
        "  return similar_dic"
      ],
      "metadata": {
        "id": "3NMhUWbZi9UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def top_repeated_courses(similar_dic,no_of_courses):\n",
        "\n",
        "  c = Counter(similar_dic)\n",
        "  most_common = c.most_common(no_of_courses)  # returns top 3 pairs\n",
        "\n",
        "  return most_common"
      ],
      "metadata": {
        "id": "vSQ94_z1psIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trending_course_content(repeated_courses,courseTitle_Newtopics):\n",
        "  with open('Occuring_courses.csv', 'w',encoding = 'UTF8', newline=\"\") as f:\n",
        "      header = [\"Course Name\",\"Jobs no.\"]\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(header)\n",
        "      for i in repeated_courses:\n",
        "        flag= True \n",
        "        for j in i:\n",
        "          j=list(i)\n",
        "          course_new_outline = courseTitle_Newtopics[j[0]]\n",
        "          #print(j)\n",
        "          l=[j[0],j[1],course_new_outline]\n",
        "          #l = [i,j]\n",
        "          if flag == True:\n",
        "            writer.writerow(l)\n",
        "            flag = False"
      ],
      "metadata": {
        "id": "hDZJJUX8WIjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_course_content(course_title_new_topics):\n",
        "  new_content = {}\n",
        "  for job_id,content_jd in course_title_new_topics.items():\n",
        "    for course_name, course_new_content in content_jd.items():\n",
        "      if course_name not in new_content:\n",
        "        new_content[course_name] = course_new_content\n",
        "      else:\n",
        "        new_content[course_name] += course_new_content+\" \"\n",
        "        better_content = delete_repetition(new_content[course_name])\n",
        "        new_content[course_name] = better_content\n",
        "  return new_content"
      ],
      "metadata": {
        "id": "6Q6OUO-Xy2WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_repetition(content):\n",
        "  unrepeated_content =' '.join(dict.fromkeys(content.split()))\n",
        "  return unrepeated_content"
      ],
      "metadata": {
        "id": "eM6Hk-u_y3nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "iahZc713Wob0",
        "outputId": "8e425b0a-1b99-47bc-8624-2f67125a3200"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ca9d70d-bc97-4494-a35e-69c7d337e36d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ca9d70d-bc97-4494-a35e-69c7d337e36d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c8e3a505405a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \"\"\"\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    116\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    117\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 118\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read properties of undefined (reading '_uploadFiles')"
          ]
        }
      ],
      "source": [
        "#Dataset of Job Descriptions\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset of Course Outlines\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded_CO = files.upload()"
      ],
      "metadata": {
        "id": "L9faSMPQE8ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlTId7koIy1h"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(io.BytesIO(uploaded['dataset.csv']), encoding= 'unicode_escape')\n",
        "JD = dataset[\"jobdescription\"]\n",
        "print(JD)\n",
        "\n",
        "\n",
        "data = cleaning_dataset(JD)\n",
        "data_words = list(sent_to_words(data))\n",
        "# print(data_words[0])\n",
        "# break\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "vector_and_features = vectorize(data_lemmatized)\n",
        "vector = vector_and_features[0]\n",
        "features = vector_and_features[1]\n",
        "\n",
        "theta_lda_component_model = topic_modeling(vector)\n",
        "\n",
        "lda_component = theta_lda_component_model[1]\n",
        "theta = theta_lda_component_model[0]\n",
        "lda_model = theta_lda_component_model[2]\n",
        "\n",
        "\n",
        "# document_per_topic(lda_component,features)\n",
        "\n",
        "# topic_per_word(theta,lda_component,features)\n",
        "\n",
        "top_words_per_document = top_20_words_per_document(theta,lda_component,features)\n",
        "\n",
        "evaluation(lda_model,vector)\n",
        "\n",
        "jd_id = JD_ID(data)\n",
        "list_of_documents = merge_documents(jd_id)\n",
        "word_list = list_of_words(theta,lda_component)\n",
        "# print(topic_coherence(word_list,list_of_documents))\n",
        "# print(JD)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(lda_model,vector)\n",
        "print(topic_coherence(word_list,list_of_documents))"
      ],
      "metadata": {
        "id": "LZBW_pZ8RaSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(top_words_per_document)"
      ],
      "metadata": {
        "id": "A5aLR6Mtta8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIktL8Dk4RSD"
      },
      "outputs": [],
      "source": [
        "course_dataset = pd.read_csv('nu_course_outlines.csv')\n",
        "print(course_dataset)\n",
        "JD_outlines = course_dataset['outlines']\n",
        "JD_title = course_dataset['Title']\n",
        "\n",
        "JD_dic = courses_dic(JD_title,JD_outlines)\n",
        "jaccard_similarity = words_from_jaccard_similarity(top_words_per_document,JD_dic)\n",
        "\n",
        "jaccard_similarity_per_JD = jaccard_similarity[0]\n",
        "jaccard_similarity_new_topics = jaccard_similarity[1]\n",
        "\n",
        "trending_courses = top_repeated_courses(most_occuring_course(jaccard_similarity_per_JD),10)\n",
        "\n",
        "courseTitle_newTopics = new_course_content(jaccard_similarity_new_topics)\n",
        "# print(courseTitle_newTopics)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "courseTitle_newTopics"
      ],
      "metadata": {
        "id": "AmyWy4Y9Xwnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(keyword_extraction_bert(courseTitle_newTopics))"
      ],
      "metadata": {
        "id": "rECrbY_0x891"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "def keyword_extraction_bert(courseTitle_newTopics):\n",
        "  keyword_dic = {}\n",
        "  for title, new_outline in courseTitle_newTopics.items():\n",
        "    kw_model = KeyBERT()\n",
        "    keywords = kw_model.extract_keywords(new_outline,keyphrase_ngram_range=(1, 2),top_n = 20)\n",
        "    keyword_dic[title] = keywords\n",
        "  return keyword_dic\n"
      ],
      "metadata": {
        "id": "Sni4GfDe2R7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "\n",
        "def keyword_extraction_yake(courseTitle_newTopics):\n",
        "  keyword_dic = {}\n",
        "  for title, new_outline in courseTitle_newTopics.items():\n",
        "  \n",
        "    kw_extractor = yake.KeywordExtractor(top = 20, n = 2)\n",
        "    keywords = kw_extractor.extract_keywords(new_outline)\n",
        "    keyword_dic[title] = []\n",
        "    for kw in keywords:\n",
        "      keyword_dic[title].append(kw) \n",
        "  return keyword_dic"
      ],
      "metadata": {
        "id": "A9wk10DXGXPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(keyword_extraction_yake(courseTitle_newTopics))"
      ],
      "metadata": {
        "id": "S-9ZmUBeHtDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}